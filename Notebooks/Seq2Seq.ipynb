{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "-qx1spdW4wrh",
        "X8EoJki3OiVg",
        "dASXhj5l3eR8",
        "ydS49wS74Qsk",
        "Oi4WUKTg4CYw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "-qx1spdW4wrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import argparse\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "kkSLOQW2Oml2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration & Utilities"
      ],
      "metadata": {
        "id": "X8EoJki3OiVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for model and training hyperparameters.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.num_samples = 20000\n",
        "        self.seq_len = 12\n",
        "        self.vocab_size = 50  # Base vocabulary size\n",
        "        self.embed_size = 128\n",
        "        self.hidden_size = 256\n",
        "        self.enc_layers = 2\n",
        "        self.dec_layers = 2\n",
        "        self.dropout = 0.25\n",
        "        self.batch_size = 128\n",
        "        self.epochs = 20\n",
        "        self.lr = 1e-3\n",
        "        self.clip = 1.0\n",
        "        self.train_split = 0.8\n",
        "        self.seed = 42\n",
        "        self.log_dir = 'runs/best_seq2seq'\n",
        "        self.checkpoint_dir = 'checkpoints'\n",
        "\n",
        "        # Special Tokens\n",
        "        self.PAD_IDX = 0\n",
        "        self.BOS_IDX = 1\n",
        "        self.EOS_IDX = 2\n",
        "\n",
        "        # Adjust vocab size to include special tokens\n",
        "        self.effective_vocab_size = self.vocab_size + 3"
      ],
      "metadata": {
        "id": "MS5YVaES4YHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Generation"
      ],
      "metadata": {
        "id": "dASXhj5l3eR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReverseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset that generates sequences and their reversed counterparts on the fly.\n",
        "    This is memory-efficient as it doesn't store all samples in memory.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, seq_len, vocab_size, bos_idx):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.bos_idx = bos_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Generate a random sequence of integers. Start from 3 to avoid special tokens.\n",
        "        src = torch.randint(3, self.vocab_size, (self.seq_len,), dtype=torch.long)\n",
        "\n",
        "        # The target is the reversed source, prefixed with a BOS token.\n",
        "        trg = torch.cat([torch.tensor([self.bos_idx]), src.flip(dims=[0])], dim=0)\n",
        "        return src, trg\n",
        "\n",
        "def create_collate_fn(pad_idx):\n",
        "    \"\"\"\n",
        "    Creates a collate function to pad sequences for batching.\n",
        "    Using pad_sequence is much more efficient than manual padding.\n",
        "    \"\"\"\n",
        "    def collate_fn(batch):\n",
        "        src_batch, trg_batch = zip(*batch)\n",
        "\n",
        "        # Get the length of each source sequence for packing later\n",
        "        src_lengths = torch.tensor([len(s) for s in src_batch], dtype=torch.long)\n",
        "\n",
        "        # Pad the sequences\n",
        "        src_padded = pad_sequence(src_batch, batch_first=True, padding_value=pad_idx)\n",
        "        trg_padded = pad_sequence(trg_batch, batch_first=True, padding_value=pad_idx)\n",
        "\n",
        "        return src_padded, trg_padded, src_lengths\n",
        "    return collate_fn"
      ],
      "metadata": {
        "id": "WLUPdrpr35vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture"
      ],
      "metadata": {
        "id": "ydS49wS74Qsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"The Encoder part of the Seq2Seq model.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=2, dropout=0.1, bidirectional=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.gru = nn.GRU(\n",
        "            embed_size, hidden_size, num_layers, batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, src, lengths):\n",
        "        \"\"\"\n",
        "        Forward pass of the encoder.\n",
        "        Uses packing to efficiently process variable-length sequences.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(src)\n",
        "        # Pack padded sequence to ignore padding in RNN computation\n",
        "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        outputs, hidden = self.gru(packed)\n",
        "        # Unpack sequence\n",
        "        outputs, _ = pad_packed_sequence(outputs, batch_first=True, total_length=src.size(1))\n",
        "\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"The Attention mechanism.\"\"\"\n",
        "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
        "        super().__init__()\n",
        "        # Attention linear layer\n",
        "        self.attn = nn.Linear((enc_hidden_size * 2) + dec_hidden_size, dec_hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hidden_size))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "        # Repeat decoder hidden state seq_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
        "\n",
        "        # Calculate energy\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        energy = energy.permute(0, 2, 1)\n",
        "\n",
        "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        weights = torch.bmm(v, energy).squeeze(1)\n",
        "\n",
        "        # CORRECTED LINE: Use a smaller value for masking that is float16-safe.\n",
        "        weights = weights.masked_fill(mask == 0, -1e4)\n",
        "\n",
        "        return torch.softmax(weights, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"The Decoder part of the Seq2Seq model.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, attention, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = attention\n",
        "        self.gru = nn.GRU(\n",
        "            (enc_hidden_size * 2) + embed_size, dec_hidden_size, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "        self.fc_out = nn.Linear((enc_hidden_size * 2) + dec_hidden_size + embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_step, hidden, encoder_outputs, mask):\n",
        "        embedded = self.embedding(input_step).unsqueeze(1) # [B, 1, emb_dim]\n",
        "\n",
        "        # Use the last hidden state of the top layer for attention\n",
        "        dec_hidden_for_attn = hidden[-1]\n",
        "\n",
        "        # Get attention weights and context vector\n",
        "        attn_weights = self.attention(dec_hidden_for_attn, encoder_outputs, mask).unsqueeze(1)\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)\n",
        "\n",
        "        # Concatenate embedded input and context vector\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.gru(rnn_input, hidden)\n",
        "\n",
        "        # Concatenate all vectors for the final prediction\n",
        "        pred_input = torch.cat((output, context, embedded), dim=2).squeeze(1)\n",
        "        prediction = self.fc_out(pred_input)\n",
        "\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"The main Seq2Seq model wrapper.\"\"\"\n",
        "    def __init__(self, config, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.config = config\n",
        "\n",
        "        enc_hidden_size = config.hidden_size\n",
        "        dec_hidden_size = config.hidden_size * 2 # To match bidirectional encoder\n",
        "\n",
        "        self.encoder = Encoder(config.effective_vocab_size, config.embed_size, enc_hidden_size,\n",
        "                               config.enc_layers, config.dropout, bidirectional=True)\n",
        "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
        "        self.decoder = Decoder(config.effective_vocab_size, config.embed_size, enc_hidden_size,\n",
        "                               dec_hidden_size, self.attention, config.dec_layers, config.dropout)\n",
        "\n",
        "        # Bridge layer to match encoder and decoder hidden state dimensions and directions\n",
        "        self.bridge = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
        "\n",
        "    def create_mask(self, lengths, max_len):\n",
        "        \"\"\"Creates a mask for the source sequence to ignore padding in attention.\"\"\"\n",
        "        batch_size = lengths.size(0)\n",
        "        mask = torch.arange(max_len, device=self.device).expand(batch_size, max_len)\n",
        "        return mask < lengths.unsqueeze(1)\n",
        "\n",
        "    def forward(self, src, trg, lengths, teacher_forcing_ratio=0.5):\n",
        "        batch_size, trg_len = trg.shape\n",
        "        vocab_size = self.config.effective_vocab_size\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, vocab_size, device=self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src, lengths)\n",
        "\n",
        "        # Adapt encoder's bidirectional hidden state for the decoder\n",
        "        # Concatenate forward and backward hidden states from all layers\n",
        "        hidden = hidden.view(self.config.enc_layers, 2, batch_size, -1)\n",
        "        hidden = torch.cat((hidden[:, 0, :, :], hidden[:, 1, :, :]), dim=2)\n",
        "\n",
        "        # The first input to the decoder is the <BOS> token\n",
        "        input_step = trg[:, 0]\n",
        "\n",
        "        # Create attention mask\n",
        "        mask = self.create_mask(lengths, src.size(1))\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input_step, hidden, encoder_outputs, mask)\n",
        "            outputs[:, t] = output\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            input_step = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "B5URBWDR4TOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traning & Evaluating"
      ],
      "metadata": {
        "id": "Oi4WUKTg4CYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, clip, scaler):\n",
        "    \"\"\"Trains the model for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, trg, lengths in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        # Move all tensors to the correct device\n",
        "        src, trg, lengths = src.to(model.device), trg.to(model.device), lengths.to(model.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Use mixed-precision training for efficiency\n",
        "        with autocast(device_type=model.device.type):\n",
        "            output = model(src, trg, lengths)\n",
        "            output_dim = output.shape[-1]\n",
        "            # Ignore <BOS> token in loss calculation\n",
        "            out = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg_flat = trg[:, 1:].reshape(-1)\n",
        "            loss = criterion(out, trg_flat)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer) # Unscale before clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def evaluate_epoch(model, dataloader, criterion):\n",
        "    \"\"\"Evaluates the model for one epoch.\"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg, lengths in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "            # Move all tensors to the correct device\n",
        "            src, trg, lengths = src.to(model.device), trg.to(model.device), lengths.to(model.device)\n",
        "\n",
        "            # Turn off teacher forcing for evaluation\n",
        "            output = model(src, trg, lengths, teacher_forcing_ratio=0.0)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            out = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg_flat = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(out, trg_flat)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            preds = output.argmax(2)\n",
        "            # Check for exact sequence match, ignoring <BOS>\n",
        "            correct += (preds[:, 1:] == trg[:, 1:]).all(dim=1).sum().item()\n",
        "            total += src.size(0)\n",
        "\n",
        "    return epoch_loss / len(dataloader), correct / total"
      ],
      "metadata": {
        "id": "jJSECYog4AbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Runner"
      ],
      "metadata": {
        "id": "qUWQz8_W369s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main function to run the training and evaluation process.\"\"\"\n",
        "    config = Config()\n",
        "    set_seed(config.seed)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_samples = int(config.num_samples * config.train_split)\n",
        "    val_samples = config.num_samples - train_samples\n",
        "\n",
        "    collate_fn = create_collate_fn(config.PAD_IDX)\n",
        "\n",
        "    train_ds = ReverseDataset(train_samples, config.seq_len, config.vocab_size, config.BOS_IDX)\n",
        "    val_ds = ReverseDataset(val_samples, config.seq_len, config.vocab_size, config.BOS_IDX)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # Initialize model, optimizer, and criterion\n",
        "    model = Seq2Seq(config, device).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=config.PAD_IDX)\n",
        "\n",
        "    # GradScaler for mixed-precision\n",
        "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "    # Setup for logging and saving\n",
        "    writer = SummaryWriter(log_dir=config.log_dir)\n",
        "    Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    train_losses, val_losses, val_accs = [], [], []\n",
        "\n",
        "    print(\"ðŸš€ Starting Training...\")\n",
        "    for epoch in range(1, config.epochs + 1):\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, config.clip, scaler)\n",
        "        val_loss, val_acc = evaluate_epoch(model, val_loader, criterion)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        # Log to TensorBoard\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2%}\")\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), Path(config.checkpoint_dir) / 'best_model.pt')\n",
        "            print(f\"âœ… New best model saved at epoch {epoch}\")\n",
        "\n",
        "    writer.close()\n",
        "    print(\"âœ… Training complete.\")\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(1, config.epochs + 1), train_losses, label='Train Loss')\n",
        "    plt.plot(range(1, config.epochs + 1), val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curves')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(1, config.epochs + 1), val_accs, label='Val Accuracy', color='orange')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy Curve')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "-TCgGk_Q3ex2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}